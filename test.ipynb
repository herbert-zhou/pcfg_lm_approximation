{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCFG:\n",
    "    def __init__(self):\n",
    "        self.rules = defaultdict(list)\n",
    "        self.non_terminals = set()\n",
    "        self.terminals = set()\n",
    "        self.start_symbol = None\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path: str):\n",
    "        pcfg = cls()\n",
    "        all_symbols = set()\n",
    "        \n",
    "        # First pass: collect all non-terminals (LHS of rules)\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split()\n",
    "                prob = float(parts[0])\n",
    "                lhs = parts[1] # LHS is the second part of the rule\n",
    "                rhs = tuple(parts[2:])\n",
    "                \n",
    "                pcfg.non_terminals.add(lhs)\n",
    "                all_symbols.add(lhs)\n",
    "                all_symbols.update(list(rhs))  # Add all RHS symbols\n",
    "                \n",
    "                pcfg.rules[lhs].append((prob, rhs))\n",
    "                \n",
    "        pcfg.terminals = all_symbols - pcfg.non_terminals\n",
    "        \n",
    "        # Determine start symbol (first non-terminal in first rule)\n",
    "        if pcfg.rules:\n",
    "            pcfg.start_symbol = next(iter(pcfg.rules.keys()))\n",
    "        \n",
    "        return pcfg\n",
    "    \n",
    "    def generate(self, symbol: str = None, max_depth: int = 10) -> List[str]:\n",
    "        if symbol is None:\n",
    "            symbol = self.start_symbol\n",
    "        if max_depth <= 0:\n",
    "            return []\n",
    "            \n",
    "        if symbol in self.terminals:\n",
    "            return [symbol]\n",
    "            \n",
    "        # Select a rule randomly according to probabilities\n",
    "        possible_rules = self.rules.get(symbol, [])\n",
    "        if not possible_rules:\n",
    "            return []\n",
    "            \n",
    "        probs, rhs_list = zip(*possible_rules)\n",
    "        probs = np.array(probs)\n",
    "        probs /= probs.sum()  # (redundant step for a well-defined PCFG) Normalize probabilities\n",
    "        \n",
    "        selected_idx = np.random.choice(len(possible_rules), p=probs)\n",
    "        selected_rhs = rhs_list[selected_idx]\n",
    "        \n",
    "        result = []\n",
    "        for s in selected_rhs:\n",
    "            result.extend(self.generate(s, max_depth-1))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCFGDataset(Dataset):\n",
    "    def __init__(self, pcfg: PCFG, num_samples: int, max_length: int, \n",
    "                 token_to_idx: Dict[str, int], special_tokens: Dict[str, int]):\n",
    "        self.pcfg = pcfg\n",
    "        self.max_length = max_length\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.special_tokens = special_tokens\n",
    "        self.samples = []\n",
    "        \n",
    "        # Generate samples\n",
    "        for _ in range(num_samples):\n",
    "            while True:\n",
    "                sequence = self.pcfg.generate(max_depth=20)\n",
    "                if 0 < len(sequence) <= max_length:\n",
    "                    break\n",
    "            self.samples.append(sequence)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.samples[idx]\n",
    "        # Convert to token indices\n",
    "        token_ids = [self.token_to_idx[token] for token in sequence]\n",
    "        \n",
    "        # Add BOS and EOS tokens\n",
    "        token_ids = [self.special_tokens['bos']] + token_ids + [self.special_tokens['eos']]\n",
    "        \n",
    "        # Create input and target (shifted by one)\n",
    "        x = torch.tensor(token_ids[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(token_ids[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "def create_token_mappings(pcfg: PCFG, special_tokens: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"Create mappings between tokens and indices.\"\"\"\n",
    "    # All terminals become tokens\n",
    "    tokens = sorted(pcfg.terminals)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    token_to_idx = {token: idx + len(special_tokens) for idx, token in enumerate(tokens)}\n",
    "    \n",
    "    # Add special tokens\n",
    "    for token, idx in special_tokens.items():\n",
    "        token_to_idx[token] = idx\n",
    "    \n",
    "    idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "    \n",
    "    return token_to_idx, idx_to_token\n",
    "\n",
    "def pcfg_collate_fn(batch, pad_idx: int):\n",
    "    \"\"\"Collate function for DataLoader that handles padding.\"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    \n",
    "    # Find max length in this batch\n",
    "    max_len = max(x.size(0) for x in xs)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_xs = []\n",
    "    padded_ys = []\n",
    "    masks = []\n",
    "    \n",
    "    for x, y in zip(xs, ys):\n",
    "        pad_len = max_len - x.size(0)\n",
    "        \n",
    "        # Pad input and target\n",
    "        padded_x = torch.cat([x, torch.full((pad_len,), pad_idx, dtype=torch.long)])\n",
    "        padded_y = torch.cat([y, torch.full((pad_len,), -1, dtype=torch.long)])  # -1 will be ignored in loss\n",
    "        \n",
    "        # Create padding mask (1 for real tokens, 0 for padding)\n",
    "        mask = torch.cat([\n",
    "            torch.ones(x.size(0), dtype=torch.long),\n",
    "            torch.zeros(pad_len, dtype=torch.long)\n",
    "        ])\n",
    "        \n",
    "        padded_xs.append(padded_x)\n",
    "        padded_ys.append(padded_y)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return torch.stack(padded_xs), torch.stack(padded_ys), torch.stack(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Register causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Project to query, key, value\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, T]\n",
    "            att = att.masked_fill(padding_mask == 0, float('-inf'))\n",
    "            \n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        \n",
    "        # Combine heads\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        x = x + self.attn(self.ln_1(x), padding_mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Token embeddings\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        \n",
    "        # Language model head\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Weight tying AFTER initialization\n",
    "        self.lm_head.weight = self.transformer.wte.weight  # Tie weights\n",
    "        \n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None, padding_mask=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        \n",
    "        # Position embeddings\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, padding_mask)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), \n",
    "                                 targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    # def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "    #     # Separate parameters with weight decay and those without\n",
    "    #     decay = set()\n",
    "    #     no_decay = set()\n",
    "    #     whitelist_modules = (nn.Linear,)\n",
    "    #     blacklist_modules = (nn.LayerNorm, nn.Embedding)\n",
    "    #     for mn, m in self.named_modules():\n",
    "    #         for pn, p in m.named_parameters():\n",
    "    #             fpn = f\"{mn}.{pn}\" if mn else pn\n",
    "    #             if pn.endswith('bias'):\n",
    "    #                 no_decay.add(fpn)\n",
    "    #             elif pn.endswith('weight') and isinstance(m, whitelist_modules):\n",
    "    #                 decay.add(fpn)\n",
    "    #             elif pn.endswith('weight') and isinstance(m, blacklist_modules):\n",
    "    #                 no_decay.add(fpn)\n",
    "        \n",
    "    #     # Validate all parameters were accounted for\n",
    "    #     param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "    #     inter_params = decay & no_decay\n",
    "    #     union_params = decay | no_decay\n",
    "    #     assert len(inter_params) == 0, f\"Parameters {inter_params} made it into both sets!\"\n",
    "    #     assert len(param_dict.keys() - union_params) == 0, f\"Parameters {param_dict.keys() - union_params} not separated!\"\n",
    "        \n",
    "    #     # Create optimizer groups\n",
    "    #     optim_groups = [\n",
    "    #         {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "    #         {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "    #     ]\n",
    "    #     return torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "            if eos_token_id is not None and (idx_next == eos_token_id).any():\n",
    "                break\n",
    "                \n",
    "        return idx\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_from_bos(self, batch_size: int = 1, max_length: int = 50, return_logprobs: bool = False):\n",
    "        \"\"\"Generate sequences from BOS token with proper batch handling and logprobs\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        bos_token = self.config.special_tokens.bos\n",
    "        eos_token = self.config.special_tokens.eos\n",
    "        \n",
    "        # Initialize with BOS token for all sequences\n",
    "        generated = torch.full((batch_size, 1), bos_token, dtype=torch.long, device=device)\n",
    "        active = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # For storing logprobs\n",
    "        all_logprobs = torch.zeros(batch_size, max_length, device=device) if return_logprobs else None\n",
    "        active_indices = torch.arange(batch_size, device=device)  # Track original positions\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            if not active.any():\n",
    "                break\n",
    "                \n",
    "            # Get logits for active sequences\n",
    "            logits, _ = self(generated[active])\n",
    "            last_logits = logits[:, -1, :]  # (active_count, vocab_size)\n",
    "            \n",
    "            # Convert to probabilities and sample\n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1)  # (active_count, 1)\n",
    "            \n",
    "            # Store logprobs if requested\n",
    "            if return_logprobs:\n",
    "                log_probs = F.log_softmax(last_logits, dim=-1)\n",
    "                selected_log_probs = log_probs.gather(1, next_tokens)  # (active_count, 1)\n",
    "                all_logprobs[active_indices[active], step] = selected_log_probs.squeeze(-1)\n",
    "            \n",
    "            # Create update tensor for all sequences\n",
    "            update = torch.full((batch_size, 1), eos_token, dtype=torch.long, device=device)\n",
    "            update[active] = next_tokens\n",
    "            \n",
    "            # Append to generated sequences\n",
    "            generated = torch.cat([generated, update], dim=1)\n",
    "            \n",
    "            # Update active status\n",
    "            active &= (update.squeeze(1) != eos_token)\n",
    "        \n",
    "        # Convert to list of sequences (remove padding)\n",
    "        sequences = []\n",
    "        seq_lengths = []\n",
    "        for seq in generated:\n",
    "            eos_pos = (seq == eos_token).nonzero()\n",
    "            end_pos = eos_pos[0].item() if eos_pos.numel() > 0 else len(seq)\n",
    "            sequences.append(seq[:end_pos].tolist())\n",
    "            seq_lengths.append(end_pos)\n",
    "        \n",
    "        if return_logprobs:\n",
    "            # Trim logprobs to actual sequence lengths\n",
    "            trimmed_logprobs = []\n",
    "            for i, length in enumerate(seq_lengths):\n",
    "                if length > 1:  # At least one token after BOS\n",
    "                    trimmed_logprobs.append(all_logprobs[i, :length-1])  # Exclude BOS\n",
    "                else:\n",
    "                    trimmed_logprobs.append(torch.tensor([], device=device))\n",
    "            return sequences, trimmed_logprobs\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "    def tokens_to_text(self, token_ids_list, idx_to_token):\n",
    "        \"\"\"Convert list of token IDs to text using vocabulary mapping\"\"\"\n",
    "        return [\n",
    "            [idx_to_token.get(idx, \"<unk>\") for idx in token_ids] \n",
    "            for token_ids in token_ids_list\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Eval Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Model Configuration ====================\n",
    "\n",
    "@dataclass\n",
    "class SpecialTokens:\n",
    "    pad: int = 0\n",
    "    bos: int = 1\n",
    "    eos: int = 2\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "    special_tokens: SpecialTokens = field(default_factory=SpecialTokens)\n",
    "\n",
    "# ==================== Training Loop ====================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    # Configure optimizer (with nanoGPT's version)\n",
    "    optimizer = model.configure_optimizers(\n",
    "        weight_decay=config['weight_decay'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        betas=config['betas'],\n",
    "        device_type='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Create iterator outside the loop\n",
    "    train_iter = iter(train_loader)\n",
    "    \n",
    "    for step in range(config['max_iters']):\n",
    "        # Evaluation\n",
    "        if step % config['eval_interval'] == 0 or step == config['max_iters'] - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss = evaluate_model(model, train_loader, config['device'])\n",
    "                val_loss = evaluate_model(model, val_loader, config['device'])\n",
    "                \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"step {step}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # Optional: Save model checkpoint\n",
    "                # torch.save(model.state_dict(), 'best_model.pth')\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Training step\n",
    "        try:\n",
    "            xb, yb, mask = next(train_iter)\n",
    "        except StopIteration:\n",
    "            # Reset iterator if we've exhausted the dataset\n",
    "            train_iter = iter(train_loader)\n",
    "            xb, yb, mask = next(train_iter)\n",
    "        \n",
    "        xb, yb, mask = xb.to(config['device']), yb.to(config['device']), mask.to(config['device'])\n",
    "        \n",
    "        # Forward pass\n",
    "        _, loss = model(xb, targets=yb, padding_mask=mask)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if config['grad_clip'] != 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "        \n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_items = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb, mask in data_loader:\n",
    "            xb, yb, mask = xb.to(device), yb.to(device), mask.to(device)\n",
    "            _, loss = model(xb, targets=yb, padding_mask=mask)\n",
    "            batch_size = xb.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_items += batch_size\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / total_items if total_items > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14\n",
      "Terminals: 11\n",
      "Non-terminals: 8\n"
     ]
    }
   ],
   "source": [
    "pcfg_file = \"pcfg/pcfg_bigger.txt\"  # Replace with your PCFG file\n",
    "special_tokens = {\n",
    "    'pad': 0,\n",
    "    'bos': 1,\n",
    "    'eos': 2\n",
    "}\n",
    "\n",
    "# Load PCFG\n",
    "pcfg = PCFG.from_file(pcfg_file)\n",
    "\n",
    "# Create token mappings\n",
    "token_to_idx, idx_to_token = create_token_mappings(pcfg, special_tokens)\n",
    "vocab_size = len(token_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Terminals: {len(pcfg.terminals)}\")\n",
    "print(f\"Non-terminals: {len(pcfg.non_terminals)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: let the PCFG generate sentences and check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "SEN = []\n",
    "for i in range(1000):\n",
    "    SEN.append(' '.join(pcfg.generate()))\n",
    "# sentence_counts = Counter(SEN)\n",
    "# for sentence, count in sentence_counts.items():\n",
    "#     print(f\"'{sentence}': {count}\")\n",
    "\n",
    "len(list(set(SEN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = PCFGDataset(pcfg, num_samples=10000, max_length=50, \n",
    "                            token_to_idx=token_to_idx, special_tokens=special_tokens)\n",
    "val_dataset = PCFGDataset(pcfg, num_samples=2000, max_length=50,\n",
    "                        token_to_idx=token_to_idx, special_tokens=special_tokens)\n",
    "test_dataset = PCFGDataset(pcfg, num_samples=2000, max_length=50,\n",
    "                            token_to_idx=token_to_idx, special_tokens=special_tokens)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                        collate_fn=lambda b: pcfg_collate_fn(b, pad_idx=special_tokens['pad']))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=lambda b: pcfg_collate_fn(b, pad_idx=special_tokens['pad']))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=lambda b: pcfg_collate_fn(b, pad_idx=special_tokens['pad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One epoch of training is 313 iterations/batches.\n"
     ]
    }
   ],
   "source": [
    "print(f'One epoch of training is {len(train_loader)} iterations/batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3.16M\n",
      "num decayed parameter tensors: 18, with 3,162,112 parameters\n",
      "num non-decayed parameter tensors: 34, with 13,824 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 2.8043, val loss 2.8051\n",
      "step 500: train loss 1.1467, val loss 1.1486\n",
      "step 1000: train loss 1.1405, val loss 1.1426\n",
      "step 1499: train loss 1.1471, val loss 1.1495\n",
      "Test loss: 1.1439\n"
     ]
    }
   ],
   "source": [
    "# Model config\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=50,  # Should match max_length in dataset\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=256,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "    special_tokens=SpecialTokens()\n",
    ")\n",
    "\n",
    "# Create model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Transformer(model_config).to(device)\n",
    "\n",
    "# Training config\n",
    "train_config = {\n",
    "    'max_iters': 1500,\n",
    "    'eval_interval': 500,\n",
    "    'learning_rate': 6e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'betas': (0.9, 0.95),\n",
    "    'device': device,\n",
    "    'grad_clip': 1.0\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, train_config)\n",
    "\n",
    "# Test the model\n",
    "test_loss = evaluate_model(model, test_loader, device)\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example generation:\n",
      "Input: bos ravens visited Alex by Alex pad pad pad\n",
      "Generated: bos ravens visited Alex by Alex pad pad pad visited ravens eos\n"
     ]
    }
   ],
   "source": [
    "# Example generation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample from test set\n",
    "    x, y, _ = next(iter(test_loader))\n",
    "    x = x[:1].to(device)  # Take first sample from batch\n",
    "    \n",
    "    # Generate continuation\n",
    "    generated = model.generate(\n",
    "        x,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.8,\n",
    "        eos_token_id=special_tokens['eos']\n",
    "    )\n",
    "    \n",
    "    # Convert to tokens\n",
    "    input_tokens = [idx_to_token[idx.item()] for idx in x[0] if idx.item() in idx_to_token]\n",
    "    generated_tokens = [idx_to_token[idx.item()] for idx in generated[0] if idx.item() in idx_to_token]\n",
    "    \n",
    "    print(\"\\nExample generation:\")\n",
    "    print(\"Input:\", \" \".join(input_tokens))\n",
    "    print(\"Generated:\", \" \".join(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: bos ravens met ravens\n",
      "Generated: bos a horses met Alex by horses\n",
      "Generated: bos the ravens helped Alex\n",
      "Generated: bos dogs helped Alex by horses\n",
      "Generated: bos Alex helped the horses by the ravens\n",
      "Generated: bos Alex visited the horses by the ravens\n",
      "Generated: bos the horses met horses\n",
      "Generated: bos a horses helped Alex\n",
      "Generated: bos a horses met the ravens\n",
      "Generated: bos a horses met a ravens by the horses\n",
      "Generated: bos a horses helped a horses near the horses\n",
      "Generated: bos Alex helped ravens near ravens\n",
      "Generated: bos a horses visited horses by ravens\n",
      "Generated: bos Alex met Alex\n",
      "Generated: bos Alex visited the horses\n"
     ]
    }
   ],
   "source": [
    "sequences = model.generate_from_bos(batch_size=15)\n",
    "texts = model.tokens_to_text(sequences, idx_to_token)\n",
    "for text in texts:\n",
    "    print(\"Generated:\", \" \".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: bos horses met the horses\n",
      "Logprobs: tensor([-2.3015, -0.9486, -1.0272, -1.0530])\n",
      "Sentence: bos the ravens met the ravens\n",
      "Logprobs: tensor([-1.0115, -0.5889, -0.9564, -1.0277, -0.5770])\n",
      "Sentence: bos a horses met the ravens\n",
      "Logprobs: tensor([-1.9288, -1.0398, -0.9567, -1.0277, -0.5777])\n",
      "Sentence: bos a horses visited Alex near a ravens\n",
      "Logprobs: tensor([-1.9288, -1.0398, -1.2323, -1.4045, -1.3562, -1.9529, -0.5804])\n",
      "Sentence: bos the ravens visited Alex by horses\n",
      "Logprobs: tensor([-1.0115, -0.5889, -1.2327, -1.4045, -1.1797, -2.2510])\n"
     ]
    }
   ],
   "source": [
    "sequences, logprobs = model.generate_from_bos(\n",
    "    batch_size=5, \n",
    "    max_length=15,\n",
    "    return_logprobs=True\n",
    ")\n",
    "texts = model.tokens_to_text(sequences, idx_to_token)\n",
    "for text, probs in zip(texts, logprobs):\n",
    "    print(\"Sentence:\", \" \".join(text))\n",
    "    print(\"Logprobs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformerLens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
